{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "%pip install pyspark\n",
    "%pip install pandas\n",
    "%pip install pyarrow\n",
    "%pip install grpcio\n",
    "%pip install google\n",
    "%pip install protobuf\n",
    "%pip install grpcio_status\n",
    "%pip install --upgrade pyspark-stubs # for IntelliSense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list | grep pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan, lit, format_number\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark connect\n",
    "__Workloads are easier to maintain__  \n",
    "Local library versions don't need to match the production servers. Essentialy, client query is converted into a unresolved logical plan which gets passed to spark server  \n",
    "\n",
    "__Lets you build Spark Connect Clients in non-JVM languages__  \n",
    "Decouples the client and the Spark Driver so that you can write a Spark Connect Client in any langugage. Examples\n",
    "- Spark Connect Python\n",
    "- Spark Connect Go\n",
    "- Spark Connect Rust\n",
    "\n",
    "__Allows for better remote development and testing__  \n",
    "To work with Spark locally, you either need a local server or SSH connection to the remote Spark Driver. This removes the need for having SSH connection to remote Spark Driver, you can connect direction. More convenient and secure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier to maintain, your local library versions (spark,etc.) don't need to match to production servers\n",
    "# essentialy client query is converted to a unresolved logical plan which gets passed to spark server\n",
    "\n",
    "# interrupted after 5 mins, this never started...something to explore in future\n",
    "\n",
    "# sparkConnect = SparkSession.builder\\\n",
    "#     .remote(\"sc://localhost\")\\\n",
    "#     .appName(\"Practice\")\\\n",
    "#     .getOrCreate()\n",
    "# sparkConnect\n",
    "\n",
    "#sparkConnect.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Practice\")\\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level data interactions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1\n",
    "- reading csv\n",
    "- types and schema\n",
    "- select, describe, creating/renaming a column, dropping columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a csv file\n",
    "houseDf = spark.read\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .csv('data/train.csv')\n",
    "houseDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(houseDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.select(['Id', 'MSSubClass']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf = houseDf.withColumn('SalePriceIncreased', houseDf['SalePrice']+1000)\n",
    "houseDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf = houseDf.drop('SalePriceIncreased')\n",
    "houseDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.withColumnRenamed('SalePrice', 'SalePriceNew').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 - Handling missing values\n",
    "- dropping rows/columns\n",
    "- parameters in dropping functionalities\n",
    "- handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before counting null values, we need to change 'NA' strings to null values\n",
    "houseDf = houseDf.na.replace('NA', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullMetrics(df):\n",
    "  # now let's count null values\n",
    "  totalRows = df.count()\n",
    "\n",
    "  # when function takes (condtion, value), if no value is passed, it'll be 1 or 0 based on condition, but with value passed, it'll return the value in column. \n",
    "  # So essentially difference between counting 1s or counting null values. returing value is preferred, in case aggregation type needs to change in future\n",
    "  nullCounts = df.select([count(when(col(c).isNull() | isnan(c), c))\n",
    "                                .alias(c) for c in df.columns])\\\n",
    "                                  .collect()[0] # bringing result from distributed DataFrame to the driver as a list of Row objects\n",
    "\n",
    "  nullCountsDf = spark.createDataFrame([nullCounts])\n",
    "  nullCountsDf.show()\n",
    "  nullPercentDf = nullCountsDf.select(*[format_number((col(c)/lit(totalRows)),2).alias(c)\n",
    "                                        for c in nullCountsDf.columns])\n",
    "  nullPercentDf.show()\n",
    "\n",
    "  return nullCountsDf, nullPercentDf\n",
    "\n",
    "nullMetrics(houseDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('total records:', houseDf.count())\n",
    "print('dropping rows with any null values:', houseDf.na.drop().count())  \n",
    "print('dropping rows with all null values:', houseDf.na.drop(how='all').count()) \n",
    "print('dropping rows with null values > than thresh:', houseDf.na.drop(thresh=2).count()) \n",
    "\n",
    "print('dropping rows with any null values in subset of columns', houseDf.na.drop(how='any', subset=['Alley']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the missing values\n",
    "houseDf.na.fill('Missing Values').show()\n",
    "houseDf.na.fill('Missing Values', ['Alley']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first gotta have the right column type\n",
    "houseDf = houseDf.withColumn(\"LotFrontage\", col(\"LotFrontage\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Imputer function for filling in missing values\n",
    "missingCols = ['LotFrontage']\n",
    "imputer = Imputer(inputCols=missingCols,\n",
    "                  outputCols=[\"{}_imputed\".format(c) for c in missingCols])\\\n",
    "                  .setStrategy(\"mean\")\n",
    "\n",
    "houseDf = imputer.fit(houseDf).transform(houseDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullMetrics(houseDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
